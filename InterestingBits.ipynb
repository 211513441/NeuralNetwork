{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned before, this project is not meant to be a state of the art, blazing fast implementation of a neutral network. There is one optimization that I decided to implement to make it at least decent, i.e. the vectorization of matrix operations.\n",
    "\n",
    "Given an input $x \\in \\mathbb{R}^n$, where $n$ is the input dimension, we know that the output is given by passing the input through the network, namely\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "z^1 &= x, \\\\\n",
    "a^l &= W^l \\cdot z^{(l-1)} + b^l, ~~~ &l = 2, 3 , \\dots, L\\\\\n",
    "z^ l &= \\sigma(a^l), ~~~&l = 2, 3 , \\dots, L\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Where $W^l, b^l$ represent the weight and biases of level $l$ respectively, and $\\sigma$ is the non-linear activation function.  \n",
    "\n",
    "We are using stochastic gradient descent with mini-batch size of $k$, which means we are go through $k$ examples before updating weights and biases. Instead of looping though each example (which would require $k$ matrix multiplications) we create a matrix of dimensions $(k, n)$, where each column is a data point. In that way, we need only one matrix multiplication for each mini-batch. In this scenario the above equations have to be interpreted as multiplications between matrices rather than matrix-vector.\n",
    "\n",
    "In our code vectorization is handled in a few places. First of all both the input data and the labels are handled as `np.array` rather than simple Python lists. Such arrays are transposed in order to be consistent with the literature (and the above equations). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data = np.array(train_data)\n",
    "train_labels = np.array(train_labels)\n",
    "self.data = train_data.T\n",
    "self.target = train_labels.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same applies to the testing data and labels (if provided):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if test_data is not None and test_labels is not None:\n",
    "    self.test_data = np.array(test_data).T\n",
    "    self.test_labels = np.array(test_labels).T\n",
    "    self.testing_error = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feed forward computations can be computed via"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for w, b in zip(self.weights, self.biases):\n",
    "    result = np.dot(w, result) + b\n",
    "    self.outputs.append(result)\n",
    "    result = self.activation(result)\n",
    "    self.activations.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this phase saving the outputs and the activations is not stricly necessary, but we will need them later when computing the coefficient for the back propagation.\n",
    "\n",
    "Note that there is also the option to absorb the biases into the weight matrix by adding them as a column (and a row of ones in the example matrix). I have decided not to do that to keep the code clearer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the deltas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to make our network learn, we are going to use the backpropagation algorithm, which means we need a way to calculate the derivative of the cost function with respect to weights and biases. \n",
    "\n",
    "Let's start by stating the cost function $C$, in our case the mean squared error:\n",
    "\n",
    "$$ \n",
    "C = \\frac{1}{2} \\sum_j (y_j - a_j)^2\n",
    "$$\n",
    "\n",
    "Here the $y_j$ are the target values and $a_j$ the corresponding activations (i.e. the network predictions).\n",
    "\n",
    "Since we are using the sigmoid function $1/(1 - exp(x))$  as activation function, we know that, for each $l = 2, \\dots, L$, the gradient of $C$ is given by\n",
    "\n",
    "$$\\begin{align}\n",
    "\\frac{\\partial C}{\\partial b^l_j} & = \\delta^l_j, \\\\\n",
    "\\frac{\\partial C}{\\partial w^l_{j k}} & = a^{l-1}_k \\cdot \\delta^l_j.\n",
    "\\end{align}\n",
    "$$\n",
    "And the deltas are calculated via\n",
    "\n",
    "$$\\begin{align}\n",
    "\\delta^L & = \\nabla_a C \\cdot \\sigma '(z^L), \\\\\n",
    "\\delta^l & = ((w^{l+1})^T \\delta^{l+1}) \\odot \\sigma'(z^l).\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Note that all the previous are matrix equalities, including the deltas. In particular  $\\delta^{L}$ has dimensions $(f, k)$, where $f$ is the dimension of the output layer and $k$ is the mini-batch dimension. In a similar way $\\delta^l$ has the same shape of the outputs (or activations) at level $l$, i.e. $(d, k)$, with $d$ being the dimension of level $l$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating the weights and biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have calculated the deltas for all levels, we need to update both weights and biases\n",
    "\n",
    "When updating biases we need to sum over all the example (i.e. sum all over the columns)\n",
    "\n",
    "$$ b^l = b^l -  \\frac{\\eta}{k} \\sum_{x}(\\delta^{x,L}),\n",
    "$$\n",
    "\n",
    "where $\\eta$ is the learning rate, $k$ is the dimension of the mini-batch and $\\delta^{x, L}$ is the column of the matrix $\\delta^L$ corresponding to the input $x$. Basically we are subtracting from $b^l$ a (scaled) sum of the columns of $\\delta^{x, L}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "self.biases = [b - (learning_rate/total)* (np.sum(d, axis=1)).resape(b.shape) \\\n",
    "               for b, d  in zip(self.biases, self.deltas)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `np.sum(d, axis=1)` performs colum-wise summation.\n",
    "\n",
    "Note that `.reshape(b.shape)` is fundamental, and was the source of a very hard bug to find.  `numpy` was broadcasting the result of the summation in a unwanted way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a similar fashion we update the weights via \n",
    "\n",
    "$$ w^l = \\frac{\\eta}{k} \\sum_{x}(\\delta^{x,L}\\cdot(a^{x, L-1})^T) $$\n",
    "\n",
    "in this case we don't need to explicitly sum over the columns, since this is automatically done by the matrix dot product. In the code this is performed via"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "self.weights =  [w - (eta/total) * np.dot(d, a.T) \\ \n",
    "                 for w, d, a in zip(self.weights, self.deltas, self.activations)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The activation functions need to be vectorized too. In this case, it means that when applied to an `np.array` the function is applied element wise.\n",
    "\n",
    "For the sigmoid function \n",
    "\n",
    "    1 / (1 + np.exp(-x))\n",
    "    \n",
    "this is obtained for free thanks to the fact that `np.exp` is already vectorized. \n",
    "\n",
    "In the case of the rectified linear unit, we needed to add the `@np.vectorize` decorator to obtain such an effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even if this code won't be used to train large networks, I have decided to implement some feature that I would have put in a 'real world' scenario.\n",
    "\n",
    "The `NeuralNetwork` object can be initialized by either a `list` containig the shape of the network, or a `str`, the path to a JSON file, that contains a pre-trained model ([like this one](models/mnist.json)).\n",
    "\n",
    "In case a string is passed, then we need to check that the file exists _and_ there is enough memory for all the weights. I perform both checks at once via:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if isinstance(shape_or_file, str):\n",
    "    try:\n",
    "        self.load(shape_or_file)\n",
    "    except FileNotFoundError:\n",
    "        print(self.FILENOTFOUNDERROR_MESSAGE)\n",
    "        raise\n",
    "    except MemoryError:\n",
    "        print(self.MEMORYERROR_MESSAGE)\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the `load` method checks that the JSON file contains the necessary data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load(self, file_location):\n",
    "    with open(file_location, 'r') as fp:\n",
    "        data = json.load(fp)\n",
    "    try:\n",
    "        self.shape = data[\"shape\"]\n",
    "        self.weights = [np.array(w) for w in data[\"weights\"]]\n",
    "        self.biases = [np.array(b) for b in data[\"biases\"]]\n",
    "    except KeyError as e:\n",
    "        print(\"Load failed, the json file does not contain the required key \", e)\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similary if the network is initialized with a `list`, we init all the internal variables with `np.array` of the right dimensions (containing zeroes), to make sure there is enough memory to train the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "elif isinstance(shape_or_file, list):\n",
    "            if len(shape_or_file) < 2:\n",
    "                print(self.WRONGSHAPE_MESSAGE)\n",
    "                raise ValueError\n",
    "\n",
    "            try:\n",
    "                self.shape = shape_or_file\n",
    "                self.activation = activation\n",
    "                self._init_weights()\n",
    "                self._init_biases()\n",
    "                self._init_activations()\n",
    "                self._init_outputs()\n",
    "                if dropout:\n",
    "                    self._init_dropouts()\n",
    "            except MemoryError:\n",
    "                print(self.MEMORYERROR_MESSAGE)\n",
    "                raise\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
